{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data set....\n",
      "Done reading....\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data set....\")\n",
    "training_data_set = pd.read_csv(\"/Users/prajwalkrishn/Desktop/My_Computer/project - Dsci 601/Offensive_Tweet_Detection/Dataset/MOLID.csv\")\n",
    "print(\"Done reading....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  subtask_a subtask_b  \\\n",
       "0  This is Dubai's like Michael 's phone went pud...  Offensive       UNT   \n",
       "1  In fact, never was perceived to be thrown. Eve...  Offensive       TIN   \n",
       "2  Bhosadi I am your mother's husband. Look at yo...  Offensive       TIN   \n",
       "3  If you ask a dog? And the smoke is drawn throu...  Offensive       TIN   \n",
       "4  Where's Ram Kadam went to talk to the BJP and ...  Offensive       TIN   \n",
       "\n",
       "  subtask_c  \n",
       "0       NaN  \n",
       "1       IND  \n",
       "2       IND  \n",
       "3       IND  \n",
       "4       GRP  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = training_data_set[[\"tweet\"]]\n",
    "level_A_labels = training_data_set[[\"subtask_a\"]]\n",
    "level_B_labels = training_data_set.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "level_C_labels = training_data_set.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
    "\n",
    "All_Cleaned_tweets = copy.deepcopy(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  This is Dubai's like Michael 's phone went pud...\n",
       "1  In fact, never was perceived to be thrown. Eve...\n",
       "2  Bhosadi I am your mother's husband. Look at yo...\n",
       "3  If you ask a dog? And the smoke is drawn throu...\n",
       "4  Where's Ram Kadam went to talk to the BJP and ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subtask_a\n",
       "0  Offensive\n",
       "1  Offensive\n",
       "2  Offensive\n",
       "3  Offensive\n",
       "4  Offensive"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_A_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_b\n",
       "0       UNT\n",
       "1       TIN\n",
       "2       TIN\n",
       "3       TIN\n",
       "4       TIN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_B_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c\n",
       "1       IND\n",
       "2       IND\n",
       "3       IND\n",
       "4       GRP\n",
       "5       IND"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_C_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  This is Dubai's like Michael 's phone went pud...\n",
       "1  In fact, never was perceived to be thrown. Eve...\n",
       "2  Bhosadi I am your mother's husband. Look at yo...\n",
       "3  If you ask a dog? And the smoke is drawn throu...\n",
       "4  Where's Ram Kadam went to talk to the BJP and ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Cleaned_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prajwalkrishn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prajwalkrishn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "lancaster = LancasterStemmer()\n",
    "wordNet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_webTags_UserNames_Noise(tweet):\n",
    "    things_to_be_removed_from_tweets = ['URL','@USER','\\'ve','n\\'t','\\'s','\\'m']\n",
    "    \n",
    "    for things in things_to_be_removed_from_tweets:\n",
    "        tweet = tweet.replace(things,'')\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "def stop_words_removal(tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for token in tokens:\n",
    "        if token not in stop:\n",
    "            if token.replace(' ','') != '':\n",
    "                if len(token) > 1:\n",
    "                    cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def tokenize(tweet):\n",
    "    lower_cased_tweet = tweet.lower()\n",
    "    return word_tokenize(lower_cased_tweet)\n",
    "\n",
    "def stemming(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = lancaster.stem(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wordNet.lemmatize(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean...: 100%|██████████| 2499/2499 [00:00<00:00, 60309.94it/s]\n",
      "Tokenize..: 100%|██████████| 2499/2499 [00:00<00:00, 10568.16it/s]\n",
      "remove STOPWORDS...: 100%|██████████| 2499/2499 [00:00<00:00, 11962.09it/s]\n",
      "Stemming...: 100%|██████████| 2499/2499 [00:00<00:00, 12301.99it/s]\n",
      "Lemmatize...: 100%|██████████| 2499/2499 [00:00<00:00, 38243.84it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc = \"clean...\")\n",
    "All_Cleaned_tweets['tweet'] = tweets['tweet'].progress_apply(remove_webTags_UserNames_Noise)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tweet'].progress_apply(tokenize)\n",
    "\n",
    "tqdm.pandas(desc=\"remove STOPWORDS...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stop_words_removal)\n",
    "\n",
    "tqdm.pandas(desc=\"Stemming...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stemming)\n",
    "\n",
    "tqdm.pandas(desc=\"Lemmatize...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(lemmatization)\n",
    "\n",
    "text_vector = All_Cleaned_tweets['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai like Michael  phone went pudica</td>\n",
       "      <td>[duba, lik, michael, phon, went, pudic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact  never was perceived to be thrown  Eve...</td>\n",
       "      <td>[fact, nev, perceiv, thrown, everyth, mov, tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother husband  Look at your...</td>\n",
       "      <td>[bhosad, moth, husband, look, moth, as, kil, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog  And the smoke is drawn throu...</td>\n",
       "      <td>[ask, dog, smok, drawn, rub, goat, alon, kang,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where Ram Kadam went to talk to the BJP and no...</td>\n",
       "      <td>[ram, kadam, went, talk, bjp, behavy, mut, con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0     This is Dubai like Michael  phone went pudica    \n",
       "1  In fact  never was perceived to be thrown  Eve...   \n",
       "2  Bhosadi I am your mother husband  Look at your...   \n",
       "3  If you ask a dog  And the smoke is drawn throu...   \n",
       "4  Where Ram Kadam went to talk to the BJP and no...   \n",
       "\n",
       "                                              tokens  \n",
       "0            [duba, lik, michael, phon, went, pudic]  \n",
       "1  [fact, nev, perceiv, thrown, everyth, mov, tow...  \n",
       "2  [bhosad, moth, husband, look, moth, as, kil, d...  \n",
       "3  [ask, dog, smok, drawn, rub, goat, alon, kang,...  \n",
       "4  [ram, kadam, went, talk, bjp, behavy, mut, con...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Cleaned_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid(text_vector):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
    "    vectorizer = vectorizer.fit(untokenized_data)\n",
    "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
    "    return vectors\n",
    "  \n",
    "def get_vectors(vectors, labels, keyword):\n",
    "    if len(vectors) != len(labels):\n",
    "        print(\"Unmatching sizes!\")\n",
    "        return\n",
    "    result = list()\n",
    "    for vector, label in zip(vectors, labels):\n",
    "        if label == keyword:\n",
    "            result.append(vector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 2499/2499 [00:00<00:00, 579668.49it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_a = tfid(text_vector) # Numerical Vectors A\n",
    "labels_level_a = level_A_labels['subtask_a'].values.tolist() # Subtask A Labels\n",
    "\n",
    "vectors_level_b = get_vectors(vectors_level_a, labels_level_a, \"Offensive\") # Numerical Vectors B\n",
    "labels_level_b = level_B_labels['subtask_b'].values.tolist() # Subtask B Labels\n",
    "\n",
    "vectors_level_c = get_vectors(vectors_level_b, labels_level_b, \"TIN\") # Numerical Vectors C\n",
    "labels_level_c = level_C_labels['subtask_c'].values.tolist() # Subtask C Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(vectors_level_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     nan,\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(labels_level_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit begins...\n",
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.9613636363636363\n",
      "Test Accuracy: 0.6938775510204082\n",
      "Confusion Matrix:\n",
      "[[ 9 20  5  0]\n",
      " [ 5 91  7  0]\n",
      " [ 3  4  2  0]\n",
      " [ 0  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.53      0.26      0.35        34\n",
      "         IND       0.78      0.88      0.83       103\n",
      "         OTH       0.14      0.22      0.17         9\n",
      "         nan       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       147\n",
      "   macro avg       0.36      0.34      0.34       147\n",
      "weighted avg       0.68      0.69      0.67       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5)\n",
    "params = {'criterion':['gini','entropy']}\n",
    "classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "classifier = classifier.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifier.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model experiment begins ...\n",
      "fit begins...\n",
      "dict_keys(['C', 'break_ties', 'cache_size', 'class_weight', 'coef0', 'decision_function_shape', 'degree', 'gamma', 'kernel', 'max_iter', 'probability', 'random_state', 'shrinking', 'tol', 'verbose'])\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.4s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.4s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.4s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.4s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.4s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.8s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.8s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.6s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.6s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.6s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.6s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.6s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   1.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   1.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   1.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   1.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.8s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   1.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   1.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   1.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   1.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   1.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   1.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   1.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   1.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   1.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   32.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.9795454545454545\n",
      "Test Accuracy: 0.7210884353741497\n",
      "Confusion Matrix:\n",
      "[[  5  17   0]\n",
      " [  7 101   0]\n",
      " [  6  11   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.28      0.23      0.25        22\n",
      "         IND       0.78      0.94      0.85       108\n",
      "         OTH       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.72       147\n",
      "   macro avg       0.35      0.39      0.37       147\n",
      "weighted avg       0.62      0.72      0.66       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifiersvc = SVC()\n",
    "print(classifiersvc.get_params().keys())\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "classifierGrid = GridSearchCV(classifiersvc, param_grid, refit = True, verbose=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model experiment begins ...\n",
      "fit begins...\n",
      "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. max_features=auto, n_estimators=200, total=   2.1s\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. max_features=auto, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=200, total=   1.0s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=700, total=   1.2s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV] .............. max_features=auto, n_estimators=700, total=   1.8s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=200, total=   0.6s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=200, total=   0.6s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=200, total=   0.7s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=700, total=   1.6s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=700, total=   1.3s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV] .............. max_features=sqrt, n_estimators=700, total=   1.4s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=200, total=   0.4s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=200, total=   0.4s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=700, total=   1.2s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=700, total=   1.2s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=700, total=   1.1s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=700, total=   1.5s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV] .............. max_features=log2, n_estimators=700, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   29.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.9954545454545455\n",
      "Test Accuracy: 0.7619047619047619\n",
      "Confusion Matrix:\n",
      "[[  4  26   1]\n",
      " [  0 108   0]\n",
      " [  1   7   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.80      0.13      0.22        31\n",
      "         IND       0.77      1.00      0.87       108\n",
      "         OTH       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.76       147\n",
      "   macro avg       0.52      0.38      0.36       147\n",
      "weighted avg       0.73      0.76      0.68       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierRFC = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "print(classifierRFC.get_params().keys())\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 700],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierRFC, param_grid, refit = True, verbose=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB model experiment begins ...\n",
      "fit begins...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.9295454545454546\n",
      "Test Accuracy: 0.6394557823129252\n",
      "Confusion Matrix:\n",
      "[[ 5 25  0  0]\n",
      " [ 9 89  0  1]\n",
      " [ 3 15  0  0]\n",
      " [ 0  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.29      0.17      0.21        30\n",
      "         IND       0.69      0.90      0.78        99\n",
      "         OTH       0.00      0.00      0.00        18\n",
      "         nan       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64       147\n",
      "   macro avg       0.25      0.27      0.25       147\n",
      "weighted avg       0.52      0.64      0.57       147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=2)]: Done  40 out of  40 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"MNB model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierMNB = MultinomialNB()\n",
    "# print(classifierMNB.get_params().keys())\n",
    "param_grid = { \n",
    "    'alpha': [1, 10, 100, 1000],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierMNB, param_grid, refit = True, verbose=2, n_jobs=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model experiment begins ...\n",
      "fit begins...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  70 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:   21.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.7522727272727273\n",
      "Test Accuracy: 0.7210884353741497\n",
      "Confusion Matrix:\n",
      "[[ 7 25  0  0]\n",
      " [ 4 99  0  0]\n",
      " [ 2  9  0  0]\n",
      " [ 0  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.54      0.22      0.31        32\n",
      "         IND       0.74      0.96      0.84       103\n",
      "         OTH       0.00      0.00      0.00        11\n",
      "         nan       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72       147\n",
      "   macro avg       0.32      0.29      0.29       147\n",
      "weighted avg       0.63      0.72      0.65       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierKNN = KNeighborsClassifier()\n",
    "#print(classifierKNN.get_params().keys())\n",
    "param_grid = { \n",
    "    'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierKNN, param_grid, refit = True, verbose=2, n_jobs=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
