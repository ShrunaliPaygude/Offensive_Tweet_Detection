{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data set....\n",
      "Done reading....\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data set....\")\n",
    "training_data_set = pd.read_csv(\"/Users/prajwalkrishn/Desktop/My_Computer/project - Dsci 601/Offensive_Tweet_Detection/Dataset/OLIDv1.0/olid-training-v1.0.tsv\",sep='\\t',header=0)\n",
    "print(\"Done reading....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = training_data_set[[\"tweet\"]]\n",
    "level_A_labels = training_data_set[[\"subtask_a\"]]\n",
    "level_B_labels = training_data_set.query(\"subtask_a == 'OFF'\")[[\"subtask_b\"]]\n",
    "level_C_labels = training_data_set.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
    "\n",
    "All_Cleaned_tweets = copy.deepcopy(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prajwalkrishn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prajwalkrishn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "lancaster = LancasterStemmer()\n",
    "wordNet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_webTags_UserNames_Noise(tweet):\n",
    "    things_to_be_removed_from_tweets = ['URL','@USER','\\'ve','n\\'t','\\'s','\\'m']\n",
    "    \n",
    "    for things in things_to_be_removed_from_tweets:\n",
    "        tweet = tweet.replace(things,'')\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "def tokenize(tweet):\n",
    "    lower_cased_tweet = tweet.lower()\n",
    "    return word_tokenize(lower_cased_tweet)\n",
    "\n",
    "def stop_words_removal(tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for token in tokens:\n",
    "        if token not in stop:\n",
    "            if token.replace(' ','') != '':\n",
    "                if len(token) > 1:\n",
    "                    cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = lancaster.stem(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wordNet.lemmatize(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwalkrishn/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "clean...: 100%|██████████| 13240/13240 [00:00<00:00, 100316.64it/s]\n",
      "Tokenize..: 100%|██████████| 13240/13240 [00:01<00:00, 10402.87it/s]\n",
      "remove STOPWORDS...: 100%|██████████| 13240/13240 [00:01<00:00, 12199.79it/s]\n",
      "Stemming...: 100%|██████████| 13240/13240 [00:01<00:00, 7836.26it/s]\n",
      "Lemmatize...: 100%|██████████| 13240/13240 [00:00<00:00, 36526.14it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc = \"clean...\")\n",
    "All_Cleaned_tweets['tweet'] = tweets['tweet'].progress_apply(remove_webTags_UserNames_Noise)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tweet'].progress_apply(tokenize)\n",
    "\n",
    "tqdm.pandas(desc=\"remove STOPWORDS...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stop_words_removal)\n",
    "\n",
    "tqdm.pandas(desc=\"Stemming...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stemming)\n",
    "\n",
    "tqdm.pandas(desc=\"Lemmatize...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(lemmatization)\n",
    "\n",
    "text_vector = All_Cleaned_tweets['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid(text_vector):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
    "    vectorizer = vectorizer.fit(untokenized_data)\n",
    "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
    "    return vectors\n",
    "  \n",
    "def get_vectors(vectors, labels, keyword):\n",
    "    if len(vectors) != len(labels):\n",
    "        print(\"Unmatching sizes!\")\n",
    "        return\n",
    "    result = list()\n",
    "    for vector, label in zip(vectors, labels):\n",
    "        if label == keyword:\n",
    "            result.append(vector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 13240/13240 [00:00<00:00, 537601.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_a = tfid(text_vector) # Numerical Vectors A\n",
    "labels_level_a = level_A_labels['subtask_a'].values.tolist() # Subtask A Labels\n",
    "\n",
    "vectors_level_b = get_vectors(vectors_level_a, labels_level_a, \"OFF\") # Numerical Vectors B\n",
    "labels_level_b = level_B_labels['subtask_b'].values.tolist() # Subtask B Labels\n",
    "\n",
    "vectors_level_c = get_vectors(vectors_level_b, labels_level_b, \"TIN\") # Numerical Vectors C\n",
    "labels_level_c = level_C_labels['subtask_c'].values.tolist() # Subtask C Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], test_size=0.25)\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5)\n",
    "params = {'criterion':['gini','entropy']}\n",
    "classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "classifier = classifier.best_estimator_\n",
    "\n",
    "accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifier.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
